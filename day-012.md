## `tokenizers`
+ huggingface에서 잘 만들어주었겠거니 생각했으나 그것은 오산
+ line issue가 있음

## line issue
+ tokenizers의 BBPE는 기본적으로 "한 줄"로 전체 코퍼스를 입력해야 함
+ 이것이 너무 과도한 부하를 일으킬 수 있어서 chunk를 나누고, line-break로 구분함
+ 그런데, 이 chunk가 너무 많으면 (실험 결과, 900만 line 이상인 경우) 에러가 발생
+ 따라서 line 수를 적당하게 control 해야 함

## sample.txt
```
나라 말이 중국과 달라 문자가 서로 맞지 않아서 백성들이 말하고자 하여도 자기 뜻을 전하지 못하는 경우가 많았다.
내가 이를 안타깝게 여겨 새로 스믈 여덟자를 만들었으니, 사람마다 쉽게 배워 편하게 쓰면 좋겠다.
살어리 살어리랐다 청산에 살어리랏다. 머루랑 달래랑 먹고 청산에 살어리랏다.
```

## test code
```python
from pathlib import Path

from tap import Tap
from tokenizers import ByteLevelBPETokenizer


class ArgumentParser(Tap):
    data_path: str = "sample.txt"
    vocab_size: int = 300
    min_frequency: int = 2


def create_tokenizer(data_path, vocab_size, min_frequency):
    current_path = Path.cwd()
    print(f"current path: {current_path}")
    save_path = current_path / "results"
    if not save_path.exists():
        save_path.mkdir(parents=True)
    print(f"save_path: {save_path}")

    bbpe = ByteLevelBPETokenizer(unicode_normalizer="nfkc")
    bbpe.train(
        files=data_path,
        vocab_size=vocab_size,
        min_frequency=min_frequency,
        special_tokens=[],
    )
    bbpe.save_model(f"{save_path}")


if __name__ == "__main__":
    args = ArgumentParser().parse_args()
    create_tokenizer(args.data_path, args.vocab_size, args.min_frequency)
```

## results
+ check `results/merges.txt` and `results/vocab.json`
